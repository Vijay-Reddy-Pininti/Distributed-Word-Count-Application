# Distributed Word Count Application

A scalable, distributed application designed to process large-scale text datasets for word frequency analysis using both **Hadoop MapReduce** and **Apache Spark (PySpark)**. This project highlights optimization techniques such as combiners and memory-efficient transformations for enhanced performance.

---

## ðŸš€ Features

- **Hadoop MapReduce Implementation**
  - Designed a robust distributed engine to count word occurrences.
  - Utilized **combiners** to reduce shuffle traffic and improve performance.
  - Achieved a **30% reduction in processing time** on benchmark datasets.

- **Optimized PySpark Implementation**
  - Rebuilt using **RDD transformations** and **DataFrame API**.
  - Improved **partitioning** and **in-memory computation**.
  - Realized an additional **25% gain in execution efficiency**.

---

## ðŸ›  Technologies Used

![Apache Hadoop](https://img.shields.io/badge/Apache%20Hadoop-66CCFF?logo=apache&logoColor=white&style=flat-square)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-FDEE21?logo=apachespark&logoColor=black&style=flat-square)
![Java](https://img.shields.io/badge/Java-ED8B00?logo=java&logoColor=white&style=flat-square)
![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white&style=flat-square)
![Jupyter Notebook](https://img.shields.io/badge/Jupyter-FA0F00?logo=jupyter&logoColor=white&style=flat-square)


## Contact
[Vijay Reddy Pininti](https://www.linkedin.com/in/vijay-reddy-pininti/)

---

Feel free to contribute, raise issues, or share your feedback!

